<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>tad.ai - Transformers </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">tad.ai</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">Article</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://twitter.com/Tad__ai" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon brands fa-linkedin"><span class="label">Facebook</span></a></li>
							<li><a href="https://github.com/tad-ai" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="https://medium.com/@tad.ai" class="icon brands fa-medium"><span class="label">Instagram</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<span class="date">February 23, 2023</span>
									<h1>Attention is All You Need <br />
									(Transformers)</h1>
									<p>Mastering the Power of Attention: Understanding the Architecture Behind this <br /> Critical Neural Network Mechanism</p>
								</header>
								<div class="image main"><img src="images/pic01.jpg" alt="" /></div>
								<p>We are in the peak of artificial intelligence and everyone talks about new AI tools in these days such as ChatGPT, Midjourney etc. If you are curious about the main technology behind those AI tools, it is TRANSFORMER.</p>
								<h2>What is a transformer?</h2>
								<p>In simple words, transformer is a neural network architecture that processes inputs sequentially, in a self-attention manner. Transformer was introduced in the paper Attention is All You Need by Ashish Vaswani et al. in 2017. The major advantage of transformer over other architectures is that it can process a whole sentence at once. This is due to the fact that transformer employs self-attention mechanism.</p>
								<p>The transformer is the first transduction model which depends entirely on the self-attention mechanism mechanism to compute its representation without using RNNs or convolution.</p>
								<span class="image fit"><img src="images/1/pic01.png"></span>
								<p>The architecture for the transformer as above mentioned image. Let's think we have English to Spanish machine translation to training the transformer.<br>
								Source = "quiero esto" <br>
									Target = "i want this"</p>
								<p>At first input batch sequence send through the input embedding and get embedding values for each word in the batch sequence. That stage we consider about the batch size, sequence length and the embedding size.  In the given example we can send "quiero esto". So for that example we have those values for the above mentioned parameters.</p>
								<p>Sequence length= 2, Batch size=1, Embedding size(d_model)= 512</p>
								<p>We get (Batch size, Sequence length, Embedding size) tensor as the output of the input embedding.</p>
								<p>After that we send it through positional encoding. This model not contains recurrence and convolution therefore we need some method to give the position of the token in the sequence. For that purpose, we need to use positional encoding. We get (Batch size,Sequence length, Embedding size) tensor as the output of the positional encoding.</p>
								<p>After that we send them through encoder. Encoder has two main layers: multi-head attention, feed forward. First send through multi-head attention and the inside of it as below mentioned image.</p>
								<span class="image fit"><img src="images/1/pic02.png"></span>
								<p>We send input embedding as key, value and query separately. The number of parallel attention heads, dimension of keys (queries) and dimension of values are h, d_k and d_v respectively. According to the paper,
									<br>h=8
									<br>d_k=d_v= d_model/h=64
								</p>
								<p>At the beginning we have (Batch size, Sequence length, Embedding size) matrix and after send them through linear layer and multiple with weights of keys,queries and values separately, we get (Batch size, Sequence length, Number of attention heads, Dimension of keys(queries or values)). We swap number of attention heads with sequence length. Now the tensor is (Batch size, Number of attention heads, Sequence length, Dimension of keys(queries or values)).</p>
								<p>Now tensors are send through scaled dot product attention to calculate attention score. The most used attention functions are additive attention and dot-product (multiplicative) attention. In here use dot-product attention because it is much faster and needs less amount of memory to execute the algorithm.</p>
								<span class="image fit"><img src="images/1/pic03.png"></span>
								<p>Above mentioned equation can be used to calculate the attention score. First query matrix ((Batch size, Number of attention heads, Sequence length, Dimension of queries) size matrix)and transpose of key matrix ((Batch size, Number of attention heads, Dimension of queries, Sequence length) size matrix) are multiplied and (Batch size, Number of attention heads, Dimension of queries, Dimension of keys) size matrix is gotten as the output. So far calculated attention score is divided by square root of embedding dimension and apply softmax along the last axis (along the 'dimension of keys' axis). Finally multiply it by value matrix and (Batch size, Number of attention heads,Sequence length, Dimension of values) size matrix is sent as the output. After that transpose that matrix into (Batch size, Sequence length, Number of attention heads, Dimension of values) and reshape it as (Batch size, Sequence length, Number of attention heads* Dimension of values).</p>
								<p>If check that attention mechanism using example matrices for only key and query matrices as shown in below figure, can be understood how it works.</p>
								<span class="image fit"><img src="images/1/pic04.png"></span>							
								<p>After that those output send through fully connected feed-forward network and for that ReLU activation function is used. At that stage (Batch size, Sequence length, Embedding size) is the output matrix size.</p>
								<p>Now let's take a look at decoder and it has three main layers: Masked multi-head attention, Multi-head attention and Feed-forward. At the very beginning, inputs for the decoder send as encoder input such as embedding of target values send after send them through output embedding and positional encoding.</p>
								<p>The process of masked multi-head attention is same as the multi-head attention process that described in above. But there is a small change in that one. After multiply query and transpose of key matrices, mask is added with them. After that the other process is same as above mentioned process and with the same sizes of the matrices.</p>
								<p>The second layer is the multi-head attention. In here output of encoder send as key and value matrices with the output of masked multi-head attention as the query matrix. Send the output through normalization layer with the query matrix as the residual connection. Finally send them through fully connected feed-forward network and send through linear and softmax layer to calculate the final probability value. 6 encoders and 6 decoders are used in original paper.</p>
								<p>The code implemenation is in here:<a href="https://github.com/tad-ai/AI-Research-Paper-Implementation/tree/main/Attention_is_all_you_need">Attention is all you need</a></p>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<form method="post" action="#">
								<div class="fields">
									<div class="field">
										<label for="name">Name</label>
										<input type="text" name="name" id="name" />
									</div>
									<div class="field">
										<label for="email">Email</label>
										<input type="text" name="email" id="email" />
									</div>
									<div class="field">
										<label for="message">Message</label>
										<textarea name="message" id="message" rows="3"></textarea>
									</div>
								</div>
								<ul class="actions">
									<li><input type="submit" value="Send Message" /></li>
								</ul>
							</form>
						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; 2023</li><li>Design: <a href="https://github.com/tad-ai">tad.ai</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>